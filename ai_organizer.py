import os
import json
import shutil
import asyncio
import re
import logging
from pathlib import Path
from datetime import datetime
from collections import defaultdict
from typing import List, Dict, Any, Optional

# ================= 0. æ—¥å¿—è®°å½•å™¨ (é»‘åŒ£å­) =================
log_file_path = 'ai_organizer_debug.log'
# å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆæ¸…ç©ºï¼ˆå¿½ç•¥å ç”¨é”™è¯¯ï¼‰
if os.path.exists(log_file_path):
    try:
        os.remove(log_file_path)
    except PermissionError:
        pass  # æ–‡ä»¶è¢«å ç”¨ï¼Œç»§ç»­æ‰§è¡Œ

logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file_path, encoding='utf-8'),
        logging.StreamHandler() # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
    ]
)

# ================= 1. æ¨¡å‹é…ç½®ä¸åˆå§‹åŒ– =================

# æ”¯æŒçš„AIæ¨¡å‹é…ç½®
MODEL_CONFIGS = {
    "gemini": {
        "name": "Google Gemini",
        "api_key_env": "GOOGLE_API_KEY",
        "model_ids": {
            "1": "models/gemini-2.5-flash-lite",  # æé€Ÿç‰ˆ
            "2": "models/gemini-2.0-flash",       # æ ‡å‡†ç‰ˆ
            "3": "models/gemini-2.5-pro",         # ä¸“ä¸šç‰ˆ
        },
        "default_model": "models/gemini-2.5-flash-lite",
        "module": "google.genai",
        "client_class": "genai.Client",
    },
    "kimi": {
        "name": "Moonshot Kimi",
        "api_key_env": "KIMI_API_KEY",
        "model_ids": {
            "1": "kimi-k2",             # â­Kimi K2 (æ¨è)
            "2": "kimi-k2.5",           # Kimi K2.5
            "3": "moonshot-v1-8k",      # 8Kä¸Šä¸‹æ–‡
            "4": "moonshot-v1-32k",     # 32Kä¸Šä¸‹æ–‡
            "5": "moonshot-v1-128k",    # 128Kä¸Šä¸‹æ–‡
        },
        "default_model": "kimi-k2.5",
        "module": "openai",
        "client_class": "OpenAI",
        "base_url": "https://api.moonshot.cn/v1",
        "async_module": "openai",
        "async_client_class": "AsyncOpenAI",
    }
}

# å…¨å±€é…ç½®å˜é‡
selected_provider: str = ""
selected_model_id: str = ""
client: Any = None
async_client: Any = None
sem: asyncio.Semaphore = asyncio.Semaphore(5)

REPORT_PATH = Path(r'./organize_audit_report.md')

# æ‰©å±•åå®šä¹‰
VIDEO_EXTS = ('.mkv', '.iso', '.ts', '.mp4', '.avi', '.rmvb', '.wmv', '.m2ts', '.mpg', '.flv', '.rm', '.m4v')
EXTRA_EXTS = ('.ass', '.srt', '.ssa', '.nfo', '.jpg', '.png')
ALL_VALID_EXTS = VIDEO_EXTS + EXTRA_EXTS


def select_model() -> tuple[str, str]:
    """
    è‡ªåŠ¨é€‰æ‹© Google Gemini æ¨¡å‹ï¼Œæ— éœ€äº¤äº’
    è¿”å›: (provider, model_id)
    """
    provider = "gemini"
    model_id = "models/gemini-2.5-flash-lite"  # ä½¿ç”¨ Gemini 2.5 Flash Lite æ¨¡å‹
    logging.info(f"è‡ªåŠ¨é€‰æ‹©æ¨¡å‹: Google Gemini ({model_id})")
    print(f"[è‡ªåŠ¨] ä½¿ç”¨æ¨¡å‹: Google Gemini ({model_id})")
    return provider, model_id


def init_client(provider: str, model_id: str) -> Any:
    """
    åˆå§‹åŒ–AIå®¢æˆ·ç«¯
    """
    global client, async_client, sem, last_request_time
    last_request_time = 0
    
    config = MODEL_CONFIGS[provider]
    # ä½¿ç”¨ç¡¬ç¼–ç çš„ API Keyï¼ˆå¼ºåˆ¶ä½¿ç”¨ï¼Œä¸å—ç¯å¢ƒå˜é‡å½±å“ï¼‰
    if provider == "gemini":
        # Gemini API Key - ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–
        api_key = os.environ.get("GOOGLE_API_KEY", "")
    else:
        # Kimi API Key
        api_key = "sk-OCqaHJpNkLykg7OmpP7c2iT8lohLmSYJux3ROzcwEmDMNTbH"
    
    if not api_key:
        raise ValueError(
            f"âŒ æœªæ£€æµ‹åˆ°ç¯å¢ƒå˜é‡ {config['api_key_env']}ã€‚\n"
            f"è¯·åœ¨ç³»ç»Ÿç¯å¢ƒå˜é‡ä¸­è®¾ç½®ï¼Œæˆ–åœ¨è¿è¡Œå‰ä½¿ç”¨:\n"
            f"  Windows: set {config['api_key_env']}=your_key\n"
            f"  Linux/Mac: export {config['api_key_env']}=your_key"
        )
    
    logging.info(f"åˆå§‹åŒ– {config['name']} å®¢æˆ·ç«¯ï¼Œæ¨¡å‹: {model_id}")
    
    if provider == "gemini":
        from google import genai
        client = genai.Client(api_key=api_key)
        # Geminiä½¿ç”¨ç›¸åŒçš„å®¢æˆ·ç«¯
        async_client = None  # å°†é€šè¿‡aioæ–¹æ³•ä½¿ç”¨
        # Geminiçš„å¹¶å‘æ§åˆ¶
        sem = asyncio.Semaphore(5)
        
    elif provider == "kimi":
        # Kimi ä½¿ç”¨ OpenAI å…¼å®¹çš„SDK
        try:
            from openai import OpenAI, AsyncOpenAI
        except ImportError:
            raise ImportError(
                "ä½¿ç”¨ Kimi æ¨¡å‹éœ€è¦å®‰è£… openai åº“ã€‚\n"
                "è¯·è¿è¡Œ: pip install openai"
            )
        
        client = OpenAI(
            api_key=api_key,
            base_url=config["base_url"]
        )
        async_client = AsyncOpenAI(
            api_key=api_key,
            base_url=config["base_url"],
            max_retries=0  # ç¦ç”¨å†…éƒ¨é‡è¯•ï¼Œæˆ‘ä»¬è‡ªå·±æ§åˆ¶
        )
        # Kimiçš„å¹¶å‘æ§åˆ¶ï¼ˆKimiçš„Rate Limitè¾ƒä¸¥æ ¼ï¼‰
        sem = asyncio.Semaphore(1)  # ä¸²è¡Œå¤„ç†ä»¥é¿å… 429 é”™è¯¯
    
    return client


def build_system_prompt() -> str:
    """
    æ„å»ºç³»ç»Ÿæç¤ºè¯ï¼ˆç”¨äºKimiç­‰æ”¯æŒsystemè§’è‰²çš„æ¨¡å‹ï¼‰
    """
    return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å½±è§†æ•´ç†ä¸“å®¶ã€‚è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š

ã€æ ¸å¿ƒå‘½åç¦ä»¤ã€‘ï¼š
1. ä¸¥ç¦ä¿®æ”¹æ–‡ä»¶åï¼šç›®æ ‡è·¯å¾„çš„æœ€åä¸€æ®µï¼ˆæ–‡ä»¶åï¼‰å¿…é¡»ä¸åŸå§‹æ–‡ä»¶åå®Œå…¨ä¸€è‡´ï¼Œä¸å¾—åšä»»ä½•æ”¹åŠ¨ã€‚

ã€ç›®å½•æ„é€ è§„åˆ™ã€‘ï¼š
1. ä¸€çº§ç›®å½•ï¼šå‰§é›†åç§° (å¹´ä»½)ã€‚
   - å¿…é¡»ä¿ç•™éå¹´ä»½æ‹¬å·ï¼šè‹¥åŸååŒ…å«åœ°åæˆ–ç‰ˆæœ¬ï¼ˆå¦‚ï¼š(åˆè‚¥)ã€(ç¾ç‰ˆ)ï¼‰ï¼Œè§†ä¸ºå‰§åä¸€éƒ¨åˆ†ä¿ç•™ã€‚
   - å¹´ä»½å¤„ç†ï¼šåªæœ‰ä»åŸå§‹æ–‡ä»¶è·¯å¾„æˆ–æ–‡ä»¶åä¸­**ç›´æ¥è¯†åˆ«å‡º** 4 ä½çº¯æ•°å­—å¹´ä»½æ‰åŠ æ‹¬å· `(å¹´ä»½)`ã€‚è‹¥æ— å¹´ä»½åˆ™ä»…ä¿ç•™å‰§åï¼Œä¸å¸¦ç©ºæ‹¬å·ã€‚**ä¸¥ç¦å‡­ç©ºæ¨æµ‹å¹´ä»½ã€‚**
2. äºŒçº§ç›®å½•ï¼š
   - å¦‚æœåŸå§‹æ–‡ä»¶è·¯å¾„ä¸­å·²ç»åŒ…å«äºŒçº§ç›®å½•,ä¸”äºŒçº§ç›®å½•åç§°æ˜ç¡®åŒ…å«äº†å­£æ•°ï¼ˆSxxï¼‰ä¿¡æ¯ï¼Œåˆ™ç›´æ¥æ²¿ç”¨åŸæ¥çš„äºŒçº§ç›®å½•åç§°,ä¸åšä»»ä½•ä¿®æ”¹;å¦åˆ™åˆ›å»ºäºŒçº§ç›®å½•
   - å¦‚æœåŸå§‹æ–‡ä»¶è·¯å¾„ä¸­å·²ç»åŒ…å«äºŒçº§ç›®å½•,åˆ™ä¸ç®¡æ”¹äºŒçº§ç›®å½•åç§°æœ‰æ²¡æœ‰å˜åŒ–,ä¸”è¯¥äºŒçº§ç›®å½•ç›®å½•ä¸‹çš„è§†é¢‘æ–‡ä»¶ä¹Ÿä¾ç„¶å½’å±åˆ°è¿™ä¸ªç›®å½•ä¸‹,
   - éœ€è¦åˆ›å»ºäºŒçº§ç›®å½•æ—¶,ä¼˜å…ˆå‘½åä¸º"å‰§å.Sxx.å¹´ä»½.ç”»è´¨.è§†é¢‘ç¼–ç æ ¼å¼.éŸ³é¢‘ç¼–ç æ ¼å¼"æ–‡ä»¶å¤¹,å…¶ä¸­Sxxä¸ºå­£æ•°ï¼ˆå¦‚æœèƒ½ä»åŸè·¯å¾„ä¸­è¯†åˆ«å‡ºå­£æ•°ï¼‰ï¼Œå¹´ä»½ã€ç”»è´¨ã€è§†é¢‘ç¼–ç æ ¼å¼ã€éŸ³é¢‘ç¼–ç æ ¼å¼ç­‰ä¿¡æ¯ä»…åœ¨åŸè·¯å¾„ä¸­æ˜ç¡®å­˜åœ¨æ—¶æ‰æ·»åŠ åˆ°äºŒçº§ç›®å½•åç§°ä¸­ï¼Œä¸”å„ä¿¡æ¯ä¹‹é—´å¿…é¡»ä½¿ç”¨ç‚¹å·ï¼ˆ.ï¼‰è¿æ¥ï¼Œä¸”äºŒçº§ç›®å½•åç§°ç»“å°¾ä¸å¾—æœ‰ç‚¹å·ã€‚**æ­¤å¤„çš„å¹´ä»½ç­‰ä¿¡æ¯ï¼Œä¸¥ç¦ä»ä¸€çº§ç›®å½•ç»§æ‰¿ï¼Œå¿…é¡»æ¥è‡ªæ–‡ä»¶è‡ªèº«çš„åŸå§‹è·¯å¾„ã€‚**
   - å¦‚æœåªèƒ½è¯†åˆ«å‡ºå­£æ•°ï¼ˆSxxï¼‰ä¿¡æ¯ï¼Œè€Œæ— æ³•è¯†åˆ«å‡ºå¹´ä»½ã€ç”»è´¨ã€è§†é¢‘ç¼–ç æ ¼å¼ã€éŸ³é¢‘ç¼–ç æ ¼å¼ç­‰å…¶ä»–ä¿¡æ¯ï¼Œåˆ™äºŒçº§ç›®å½•å‘½åä¸º"Season XX"ã€‚

ã€æœ€ç»ˆè·¯å¾„ç»“æ„ã€‘: ç›®æ ‡è·¯å¾„ `target` å¿…é¡»æ˜¯ä»¥ä¸‹ä¸¤ç§æ ¼å¼ä¹‹ä¸€ï¼š
   - `[ä¸€çº§ç›®å½•]/å‰§å.Sxx.å¹´ä»½.ç”»è´¨.è§†é¢‘ç¼–ç æ ¼å¼.éŸ³é¢‘ç¼–ç æ ¼å¼.ç»„å/[åŸå§‹æ–‡ä»¶å]` 
   - `[ä¸€çº§ç›®å½•]/Season XX/[åŸå§‹æ–‡ä»¶å]`

è¯·ç›´æ¥è¿”å› JSON æ•°ç»„æ ¼å¼ã€‚"""


def build_user_prompt(file_chunk: List[str]) -> str:
    """
    æ„å»ºç”¨æˆ·æç¤ºè¯
    """
    return f"è¯·ä¸ºä»¥ä¸‹æ–‡ä»¶åˆ—è¡¨è®¡ç®— Emby ç›®æ ‡è·¯å¾„ï¼š\n\næ–‡ä»¶åˆ—è¡¨: {json.dumps(file_chunk, ensure_ascii=False)}\n\nè¯·è¿”å›æ ¼å¼ï¼š[{{\"original\": \"...\", \"target\": \"...\", \"reason\": \"...\"}}]"


# ================= 2. æ ‘çŠ¶å›¾ä¸ç»Ÿè®¡å·¥å…· =================
def build_tree_string(paths):
    """æ„å»ºæœ‰åºçš„ç›®å½•æ ‘æ–‡æœ¬ç»“æ„"""
    tree = {}
    for path in paths:
        parts = Path(path).parts
        current_level = tree
        for part in parts:
            if part not in current_level: current_level[part] = {}
            current_level = current_level[part]

    def recurse(node, prefix=""):
        tree_str = ""
        items = sorted(node.keys())
        for i, name in enumerate(items):
            is_last = (i == len(items) - 1)
            connector = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
            tree_str += f"{prefix}{connector}{name}\n"
            new_prefix = prefix + ("    " if is_last else "â”‚   ")
            tree_str += recurse(node[name], new_prefix)
        return tree_str
    return "/TV Shows\n" + recurse(tree)

def generate_audit_stats(all_decisions):
    """ç”Ÿæˆå‰§é›†å’Œå­£åº¦çš„ç»“æ„åŒ–å®¡è®¡ç»Ÿè®¡"""
    stats = defaultdict(lambda: {"orig_sources": set(), "total_count": 0, "seasons": defaultdict(int)})
    for d in all_decisions:
        orig_p, targ_p = Path(d.get('original', '')), Path(d.get('target', ''))
        if not targ_p.parts: continue
        
        orig_root = orig_p.parts[0] if len(orig_p.parts) > 1 else "æ ¹ç›®å½•"
        show_name = targ_p.parts[0]
        season_name = targ_p.parts[1] if len(targ_p.parts) >= 2 else "ä¸€çº§ç›®å½•ç›´æ”¾"
        
        stats[show_name]["orig_sources"].add(orig_root)
        stats[show_name]["total_count"] += 1
        stats[show_name]["seasons"][season_name] += 1
    return stats


# ================= 3. å¼‚æ­¥ AI å†³ç­–é€»è¾‘ =================
async def call_gemini_api(prompt: str, model_id: str, chunk_id: int) -> List[Dict]:
    """
    è°ƒç”¨ Google Gemini API
    """
    from google import genai
    
    max_retries = 6
    wait_time = 5
    
    for attempt in range(max_retries):
        try:
            response = await client.aio.models.generate_content(model=model_id, contents=prompt)
            text = response.text
            logging.debug(f"[æ‰¹æ¬¡ {chunk_id}] æ”¶åˆ° AI åŸå§‹å“åº”:\n---\n{text}\n---")
            
            # æ¸…ç†å“åº”æ–‡æœ¬
            text = text.replace('```json', '').replace('```', '').strip()
            match = re.search(r'\[.*\]', text, re.DOTALL)
            result = json.loads(match.group()) if match else json.loads(text)
            return result
            
        except Exception as e:
            logging.error(f"[æ‰¹æ¬¡ {chunk_id}] ç¬¬ {attempt+1} æ¬¡å°è¯•å¤±è´¥: {e}")
            err_msg = str(e).lower()
            if any(x in err_msg for x in ["503", "429", "disconnected", "timeout"]):
                print(f"â³ [æ‰¹æ¬¡ {chunk_id}] ç¹å¿™/é‡è¿ï¼Œ{wait_time}s åé‡è¯• ({attempt+1}/{max_retries})...")
                await asyncio.sleep(wait_time)
                wait_time *= 2
                continue
            print(f"[X] [æ‰¹æ¬¡ {chunk_id}] è‡´å‘½é”™è¯¯: {e}")
            return []
    
    return []


async def call_kimi_api(prompt: str, system_prompt: str, model_id: str, chunk_id: int) -> List[Dict]:
    """
    è°ƒç”¨ Moonshot Kimi API (OpenAIå…¼å®¹)
    """
    global last_request_time
    max_retries = 10
    wait_time = 3  # åˆå§‹ç­‰å¾…æ—¶é—´
    
    # å…¨å±€è¯·æ±‚é—´éš”æ§åˆ¶ - æ¯ä¸ªè¯·æ±‚é—´éš”2ç§’
    import time
    elapsed = time.time() - last_request_time
    if elapsed < 2.0:
        await asyncio.sleep(2.0 - elapsed)
    
    for attempt in range(max_retries):
        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ]
            
            response = await async_client.chat.completions.create(
                model=model_id,
                messages=messages,
                temperature=0.1,  # ä½æ¸©åº¦ï¼Œæé«˜ç¡®å®šæ€§
                max_tokens=4096
            )
            
            last_request_time = time.time()
            text = response.choices[0].message.content
            logging.debug(f"[æ‰¹æ¬¡ {chunk_id}] æ”¶åˆ° AI åŸå§‹å“åº”:\n---\n{text}\n---")
            
            # æ¸…ç†å“åº”æ–‡æœ¬
            text = text.replace('```json', '').replace('```', '').strip()
            match = re.search(r'\[.*\]', text, re.DOTALL)
            result = json.loads(match.group()) if match else json.loads(text)
            return result
            
        except Exception as e:
            logging.error(f"[æ‰¹æ¬¡ {chunk_id}] ç¬¬ {attempt+1} æ¬¡å°è¯•å¤±è´¥: {e}")
            err_msg = str(e).lower()
            # Kimi/OpenAI çš„é”™è¯¯ç±»å‹
            if any(x in err_msg for x in ["rate limit", "429", "timeout", "connection", "503", "500"]):
                print(f"[ç­‰å¾…] [æ‰¹æ¬¡ {chunk_id}] ç¹å¿™/é™æµï¼Œ{wait_time}s åé‡è¯• ({attempt+1}/{max_retries})...")
                await asyncio.sleep(wait_time)
                wait_time *= 2
                continue
            print(f"[X] [æ‰¹æ¬¡ {chunk_id}] è‡´å‘½é”™è¯¯: {e}")
            return []
    
    return []


async def get_ai_decision_async(file_chunk: List[str], chunk_id: int, total_chunks: int) -> List[Dict]:
    """
    ç»Ÿä¸€çš„AIå†³ç­–æ¥å£ï¼Œæ ¹æ®é€‰æ‹©çš„æ¨¡å‹è°ƒç”¨ä¸åŒçš„API
    """
    async with sem:
        system_prompt = build_system_prompt()
        user_prompt = build_user_prompt(file_chunk)
        
        # æ„å»ºå®Œæ•´çš„promptç”¨äºæ—¥å¿—
        full_prompt = f"{system_prompt}\n\n{user_prompt}"
        logging.debug(f"[æ‰¹æ¬¡ {chunk_id}] å‘é€ç»™ AI çš„ Prompt:\n---\n{full_prompt}\n---")
        
        # æ ¹æ®æä¾›å•†è°ƒç”¨ä¸åŒçš„API
        if selected_provider == "gemini":
            # Gemini å°†systemå’Œuseråˆå¹¶
            combined_prompt = f"{system_prompt}\n\n{user_prompt}"
            result = await call_gemini_api(combined_prompt, selected_model_id, chunk_id)
        elif selected_provider == "kimi":
            result = await call_kimi_api(user_prompt, system_prompt, selected_model_id, chunk_id)
        else:
            logging.error(f"æœªçŸ¥çš„æ¨¡å‹æä¾›å•†: {selected_provider}")
            return []
        
        if result:
            logging.info(f"[OK] [AI åˆ†æ] æ‰¹æ¬¡ {chunk_id}/{total_chunks} æˆåŠŸ ({len(file_chunk)} æ–‡ä»¶)")
        
        return result


# ================= 4. æ‰«æå‡½æ•° =================
def scan_files(root_path: Path) -> tuple[List[str], List[str]]:
    """
    æ‰«æç›®å½•ä¸­çš„åª’ä½“æ–‡ä»¶
    è¿”å›: (å¤„ç†åˆ—è¡¨, è·³è¿‡çš„BDMVåˆ—è¡¨)
    """
    process_list, skipped_bdmv = [], []
    for root, dirs, files in os.walk(root_path):
        # è¯†åˆ«å¹¶è·³è¿‡ BDMV
        if 'BDMV' in [d.upper() for d in dirs] or 'index.bdmv' in [f.lower() for f in files]:
            skipped_bdmv.append(root)
            dirs[:] = []
            continue
        for file in files:
            if file.lower().endswith(ALL_VALID_EXTS):
                process_list.append(os.path.relpath(os.path.join(root, file), root_path))
    return process_list, skipped_bdmv


# ================= 5. ä¸»å¼‚æ­¥å…¥å£ =================
async def run_organizer():
    global selected_provider, selected_model_id
    
    logging.info("="*60)
    logging.info("[Emby AI] å¼‚æ­¥åª’ä½“æ•´ç†å·¥å…· v2026.Universal (Gemini + Kimi)")
    logging.info("="*60)
    
    # 0. é€‰æ‹©æ¨¡å‹
    selected_provider, selected_model_id = select_model()
    logging.info(f"ç”¨æˆ·é€‰æ‹©æ¨¡å‹: {selected_provider} - {selected_model_id}")
    
    # 1. åˆå§‹åŒ–å®¢æˆ·ç«¯
    try:
        init_client(selected_provider, selected_model_id)
        print(f"[OK] æˆåŠŸåˆå§‹åŒ– {MODEL_CONFIGS[selected_provider]['name']} å®¢æˆ·ç«¯")
    except Exception as e:
        print(f"[X] åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # 2. è·å–ç”¨æˆ·è¾“å…¥
    src_input = r"C:\Users\wiz\Desktop\aftermovetmps\Step1Tmp\tvshow"  # ç¡¬ç¼–ç æºç›®å½•
    print(f"\n1. æºç›®å½• (å¾…æ•´ç†): {src_input}")
    dst_input = r"C:\Users\wiz\Desktop\aftermovetmps\Step1Tmp\test"  # ç¡¬ç¼–ç ç›®çš„ç›®å½•
    print(f"2. ç›®çš„ç›®å½• (åª’ä½“åº“): {dst_input}")
    source_dir, target_dir = Path(src_input), Path(dst_input)
    is_dry_run = True  # è‡ªåŠ¨å¼€å¯é¢„è§ˆæ¨¡å¼
    print("[è‡ªåŠ¨] é¢„è§ˆæ¨¡å¼: å·²å¼€å¯")
    
    logging.info(f"ç”¨æˆ·è¾“å…¥ - æºç›®å½•: {source_dir}")
    logging.info(f"ç”¨æˆ·è¾“å…¥ - ç›®çš„ç›®å½•: {target_dir}")
    logging.info(f"ç”¨æˆ·è¾“å…¥ - é¢„è§ˆæ¨¡å¼: {'æ˜¯' if is_dry_run else 'å¦'}")
    
    # 3. æ‰«æ
    files, skipped_bdmv = scan_files(source_dir)
    if not files:
        logging.info("[æç¤º] æ²¡å‘ç°å¯å¤„ç†çš„æ–‡ä»¶ï¼Œä»»åŠ¡ç»“æŸã€‚")
        return
    logging.info(f"[æ‰«æ] å®Œæˆï¼šå‘ç° {len(files)} ä¸ªæ–‡ä»¶ï¼Œè·³è¿‡ {len(skipped_bdmv)} ä¸ª BDMV ç›®å½•ã€‚")
    
    # 4. é¡ºåºåˆ†æ (é¿å…APIå¹¶å‘é™åˆ¶)
    chunk_size = 20  # æ¯æ‰¹20ä¸ªæ–‡ä»¶
    chunks = [files[i:i+chunk_size] for i in range(0, len(files), chunk_size)]
    logging.info(f"[AI] æ¨¡å‹ {selected_model_id} å·²å°±ç»ªï¼Œå°†åˆ† {len(chunks)} æ‰¹æ¬¡é¡ºåºåˆ†æ...")
    
    # é¡ºåºå¤„ç†æ¯ä¸ªæ‰¹æ¬¡ï¼Œé¿å…å¹¶å‘429é”™è¯¯
    all_results = []
    for i, chunk in enumerate(chunks):
        result = await get_ai_decision_async(chunk, i+1, len(chunks))
        all_results.append(result)
        # æ‰¹æ¬¡é—´é—´éš”2ç§’
        if i < len(chunks) - 1:
            await asyncio.sleep(2)
    
    # åˆå¹¶ç»“æœå¹¶æŒ‰ç›®æ ‡è·¯å¾„å…¨å±€æ’åº
    all_decisions = [item for sublist in all_results for item in sublist]
    all_decisions.sort(key=lambda x: x.get('target', ''))
    
    if not all_decisions:
        logging.warning("âš ï¸ AI æœªè¿”å›ä»»ä½•æœ‰æ•ˆç»“æœï¼Œè¯·æ£€æŸ¥æ—¥å¿—ã€‚")
        return
    
    # 5. ç»Ÿè®¡å®¡è®¡
    audit_stats = generate_audit_stats(all_decisions)
    
    # 6. æ„å»ºæŠ¥å‘Š
    logging.info(f"[æŠ¥å‘Š] åˆ†æå®Œæˆï¼Œå…±è·å¾— {len(all_decisions)} æ¡æ•´ç†å†³ç­–ï¼Œå¼€å§‹æ„å»ºå®¡è®¡æŠ¥å‘Š...")
    target_paths = [d.get('target') for d in all_decisions if d.get('target')]
    
    # å®¡è®¡æ±‡æ€»è¡¨
    audit_table = ["| æ•´ç†åå‰§å | åŸå§‹æ¥æº | æ–‡ä»¶æ€»æ•° | å­£æ•°åŠé›†æ•°åˆ†å¸ƒ |", "| :--- | :--- | :--- | :--- |"]
    for show, data in sorted(audit_stats.items()):
        orig_src = "<br>".join(list(data["orig_sources"]))
        dist = " / ".join([f"{s}({c}é›†)" for s, c in sorted(data["seasons"].items())])
        audit_table.append(f"| **{show}** | {orig_src} | {data['total_count']} | {dist} |")
    
    report = [
        f"# åª’ä½“åº“æ•´ç†å®¡è®¡æŠ¥å‘Š ({datetime.now().strftime('%Y-%m-%d %H:%M')})",
        f"\n**ä½¿ç”¨æ¨¡å‹**: {MODEL_CONFIGS[selected_provider]['name']} ({selected_model_id})",
        "\n## 1. ç»“æ„åˆè§„æ€§å®¡è®¡è¡¨",
        "> è¯·åœ¨æ­¤æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶ä¸¢å¤±æˆ–å‰§é›†è¢«é”™è¯¯åˆå¹¶ã€‚",
        "\n".join(audit_table),
        "\n## 2. é¢„æœŸç›®å½•æ ‘é¢„è§ˆ (æœ‰åº)",
        "```text", build_tree_string(target_paths), "```",
        "\n## 3. è¯¦ç»†æ˜ å°„æ¸…å• (å·²æ’åº)",
        "| ç›®æ ‡ä½ç½® (æœ‰åº) | â¬…ï¸ åŸå§‹æ–‡ä»¶ | ç†ç”± |",
        "| :--- | :--- | :--- |"
    ]
    for d in all_decisions:
        report.append(f"| **{d.get('target')}** | {d.get('original')} | {d.get('reason')} |")
    
    with open(REPORT_PATH, 'w', encoding='utf-8') as f:
        f.write("\n".join(report))
    logging.info(f"[å®Œæˆ] å®¡è®¡æŠ¥å‘Šå·²ç”Ÿæˆï¼š{REPORT_PATH.absolute()}")
    
    # 7. ç‰©ç†æ‰§è¡Œ
    if not is_dry_run and all_decisions:
        confirm = input(f"\nâš ï¸ é¢„è§ˆå·²å®Œæˆï¼Œç¡®è®¤ç‰©ç†ç§»åŠ¨ {len(all_decisions)} ä¸ªæ–‡ä»¶ï¼Ÿ(y/n): ")
        if confirm.lower() == 'y':
            logging.info("ç”¨æˆ·ç¡®è®¤æ‰§è¡Œç‰©ç†ç§»åŠ¨ã€‚")
            moved_count = 0
            for item in all_decisions:
                src, dst = source_dir / item.get('original', ''), target_dir / item.get('target', '')
                if src.exists() and item.get('target'):
                    dst.parent.mkdir(parents=True, exist_ok=True)
                    try:
                        logging.debug(f"æ­£åœ¨ç§»åŠ¨: '{src}' -> '{dst}'")
                        shutil.move(str(src), str(dst))
                        moved_count += 1
                        if moved_count % 10 == 0:
                            logging.info(f"   ğŸšš ç§»åŠ¨è¿›åº¦: [{moved_count}/{len(all_decisions)}]")
                    except Exception as e:
                        logging.error(f"âŒ ç§»åŠ¨å¤±è´¥: ä» '{src}' åˆ° '{dst}' å¤±è´¥! é”™è¯¯: {e}")
            logging.info(f"âœ… å½’æ¡£ä»»åŠ¡åœ†æ»¡å®Œæˆï¼Œå…±ç§»åŠ¨ {moved_count} ä¸ªæ–‡ä»¶ã€‚")
        else:
            logging.info("ç”¨æˆ·å–æ¶ˆäº†ç‰©ç†ç§»åŠ¨æ“ä½œã€‚")
    
    if skipped_bdmv:
        logging.warning(f"[æç¤º] æœ‰ {len(skipped_bdmv)} ä¸ª BDMV åŸç›˜è¢«è·³è¿‡ï¼Œè¯¦æƒ…è¯·æŸ¥çœ‹æ—¥å¿—ã€‚")
        logging.debug(f"è·³è¿‡çš„ BDMV ç›®å½•åˆ—è¡¨: {skipped_bdmv}")


if __name__ == "__main__":
    asyncio.run(run_organizer())
